{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fr_lm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7z4mGibSg2bM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d3272c-c517-4ac4-cf11-8f3f14206184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "2ZWelm6bjuKF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/news-crawl-corpus/train.news', 'r') as f:\n",
        "    news_train = f.readlines()\n",
        "with open('/content/drive/MyDrive/news-crawl-corpus/dev.news', 'r') as f:\n",
        "    news_validation = f.readlines()\n",
        "with open('/content/drive/MyDrive/news-crawl-corpus/test.news', 'r') as f:\n",
        "    news_test = f.readlines()\n"
      ],
      "metadata": {
        "id": "p2HktpoilUAF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print length of corpus\n",
        "print('train:', len(news_train))  \n",
        "print('valid:', len(news_validation))"
      ],
      "metadata": {
        "id": "L7L1raBKSMlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6afe3bd8-e7fc-4613-df90-7dcf8533f6e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 20000\n",
            "valid: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess(text):\n",
        "    #the function prpocess a single line\n",
        "    #it takes the single line and returns a list of tokens for that particular line\n",
        "    #make text lower\n",
        "    cleaned_text = text.lower()\n",
        "    # cleaned_text = text\n",
        "    #remove non-ASCII characters\n",
        "    #cleaned_text = re.sub(r'[^\\x00-\\x7F]+',' ', cleaned_text)\n",
        "    # remove URLS\n",
        "    cleaned_text = re.sub(r\"http\\S+\", \"<URL>\", cleaned_text)\n",
        "    # remove HTs\n",
        "    cleaned_text = re.sub(r\"#[A-Za-z0-9_]+\", \"<HASHTAG>\", cleaned_text)\n",
        "    # remove Mentions\n",
        "    cleaned_text = re.sub(r\"@[A-Za-z0-9_]+\", \"<MENTION>\", cleaned_text)\n",
        "    #replace percentage quantities with tags\n",
        "    cleaned_text = re.sub(r'(\\d+(\\.\\d+)?%)',\"<PERCENT>\",cleaned_text)\n",
        "    #replace numbers with tags\n",
        "    cleaned_text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" <NUM> \", cleaned_text)\n",
        "    #hypenated words are accounted for by joining them/merging them together\n",
        "    cleaned_text = re.sub(r'\\w+(?:-\\w+)+', '', cleaned_text)\n",
        "    # Substitue for punctuations\n",
        "    # cleaned_text = re.sub(r\"(\\'t)\",\" not\",cleaned_text)\n",
        "    # cleaned_text = re.sub(r'(i\\'m)',\"i am\",cleaned_text)\n",
        "    # cleaned_text = re.sub(r'(ain\\'t)',\"am not\",cleaned_text)\n",
        "    # cleaned_text = re.sub(r'(\\'ll)',\" will\",cleaned_text)\n",
        "    # cleaned_text = re.sub(r'(\\'ve)',\" have\",cleaned_text)\n",
        "    # cleaned_text = re.sub(r'(\\'re)',\" are\",cleaned_text)\n",
        "    # cleaned_text = re.sub(r'(\\'s)',\" is\",cleaned_text)\n",
        "    # cleaned_text = re.sub(r'(\\'re)',\" are\",cleaned_text)\n",
        "    #removing repetetive spam\n",
        "    cleaned_text = re.sub('\\!\\!+', '!', cleaned_text)\n",
        "    cleaned_text = re.sub('\\*\\*+', '*', cleaned_text)\n",
        "    cleaned_text = re.sub('\\>\\>+', '>', cleaned_text)\n",
        "    cleaned_text = re.sub('\\<\\<+', '<', cleaned_text)\n",
        "    cleaned_text = re.sub('\\?\\?+', '?', cleaned_text)\n",
        "    cleaned_text = re.sub('\\!\\!+', '!', cleaned_text)\n",
        "    cleaned_text = re.sub('\\.\\.+', '.', cleaned_text)\n",
        "    cleaned_text = re.sub('\\,\\,+', ',', cleaned_text)\n",
        "    #matching punctuation characters at end of sentences and padding them\n",
        "    cleaned_text = re.sub('([;:.,!?()])', r' \\1 ', cleaned_text)\n",
        "    #removing multiple spaces finally\n",
        "    cleaned_text = re.sub('\\s{2,}', ' ', cleaned_text)\n",
        "    #remove trailing white spaces\n",
        "    cleaned_text = re.sub(r'\\s+$', '', cleaned_text) #important to get rid of empty tokens at the end of list\n",
        "    #tokenization based on spaces for each line\n",
        "    # spaces = r\"\\s+\"\n",
        "    # tokenized_sent = re.split(spaces, cleaned_text)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "x15Gaay6dj9Y"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the corpus\n",
        "news_train_clean = [preprocess(line) for line in news_train]\n",
        "news_validation_clean = [preprocess(line) for line in news_validation]\n",
        "news_validation_clean[:10]"
      ],
      "metadata": {
        "id": "j-pLI43ldlpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6638b91b-938b-43ed-b947-b029f9faa705"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"dans une vidéo publiée vendredi matin , le président obama affirme que la a été inondée de messages d'appui pour renforcer les lois sur les armes à feu au cours de la dernière semaine .\",\n",
              " \"une fois la canne et le moulinet bien en mains , il n'est pas rare de faire de belles prises .\",\n",
              " 'court 1 : a partir de 14h00 ( 9h30 h . f . )',\n",
              " 'comme sur les situations de coups de pied arrêtés .',\n",
              " \"monahan domine de son côté les pointeurs des 67's avec <NUM> points en <NUM> matches .\",\n",
              " 'chaque membre touchera <NUM> $ en boni de signature .',\n",
              " \"en <NUM> matchs , il a marqué <NUM> buts et ajouté <NUM> mentions d'aide pour <NUM> points .\",\n",
              " \"aujourd'hui , numéro 4 : ian poulter .\",\n",
              " \"cette année , hanouka s'est amorcée le <NUM> décembre .\",\n",
              " 'un anniversaire et un enterrement']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_test_clean = [preprocess(line) for line in news_test]"
      ],
      "metadata": {
        "id": "v7IvYK9TQMv-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_UNK_corpus(data):\n",
        "    freq_dict = {}\n",
        "    for line in data:\n",
        "        for word in line.split():\n",
        "            if word in freq_dict:\n",
        "                freq_dict[word] += 1\n",
        "            else:\n",
        "                freq_dict[word] = 1\n",
        "    freq_dict['<UNK>'] = 0\n",
        "    #replace all words with frequency less than 3 with <UNK> in freq_dict\n",
        "    word_list = []\n",
        "    for word in freq_dict:\n",
        "        if freq_dict[word] == 1:\n",
        "            freq_dict[\"<UNK>\"] += freq_dict[word]\n",
        "            #remove the word from the freq_dict\n",
        "            word_list.append(word)\n",
        "    for word in word_list:\n",
        "        del freq_dict[word]\n",
        "    #for each sentence in corpus if word not in freq_dict replace with <UNK>\n",
        "    temp = []\n",
        "    for line in data:\n",
        "        for word in line.split():\n",
        "            if word not in freq_dict:\n",
        "                line = line.replace(word, \"<UNK>\")\n",
        "        temp.append(line)\n",
        "    return temp\n",
        "news_train_clean = create_UNK_corpus(news_train_clean)\n",
        "news_train_clean[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5xQsgoy28nT",
        "outputId": "c436b074-9e17-4734-bfde-fba1a4c3d051"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"au pied des pyramides de <UNK> , l'air est exactement le même .\",\n",
              " \"l'un des effets <UNK> du cancer est la détresse psychologique .\",\n",
              " \"les thrashers , qui avaient le deuxième choix , ont donc accepté d'échanger ce choix aux canucks en retour du tout premier .\",\n",
              " \"selon lui , le président des , barack obama et la présidente de la commission d'enquête sur l'octroi et la gestion des contrats publics dans l'industrie de la construction , la juge france charbonneau , sont deux des trois personnalités qui ont occupé l'actualité , aux côtés de la première ministre du québec , pauline marois .\",\n",
              " 'une seule de ses plumes tombée à terre , avait , , suffi à <UNK> le monde . mais comment trouver sa majesté ?',\n",
              " \"en plus de cette rémunération de très nombreux avantages viendront s'y ajouter ( participation et intéressement , ce , . )\",\n",
              " \"depuis cet été , cependant , de belles choses semblent poindre à l'horizon pour le jeune homme de <NUM> ans .\",\n",
              " '\"notre but à tous les deux , c\\'est de faire rire l\\'autre et de le faire <UNK> , .',\n",
              " 'mohamed <UNK> , qui se présente comme un membre d\\'aqmi à tombouctou , a de son côté justifié ces <UNK> en affirmant que tout ce qui ne relève pas de l\\'islam , \"ce n\\'est pas bien , l\\'homme doit vénérer seulement allah\" .',\n",
              " \"l'animation de cette soirée toute spéciale sera assurée par simon philippe <UNK> .\"]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "    def get_data(self, train_data, batch_size):\n",
        "        tokens = 0\n",
        "        for line in train_data:\n",
        "            #tokenize the line\n",
        "            words = ['<START>'] + line.split() + ['<END>']\n",
        "            tokens += len(words)\n",
        "            for word in words:\n",
        "                self.dictionary.add_word(word)\n",
        "\n",
        "        #tokenizing the content\n",
        "        ids = torch.LongTensor(tokens) #contains the ids of the words\n",
        "        token = 0\n",
        "        for line in train_data:\n",
        "            words = ['<START>'] + line.split() + ['<END>']\n",
        "            for word in words:\n",
        "                ids[token] = self.dictionary.word2idx[word]\n",
        "                token += 1\n",
        "        num_batches = ids.size(0) // batch_size\n",
        "        ids = ids[:num_batches*batch_size]\n",
        "        ##return reshaped tensor using .view()\n",
        "        ## the -1-> situation that you don't know how many rows you want but are sure of the number of columns\n",
        "        return ids.view(batch_size, -1) "
      ],
      "metadata": {
        "id": "0fh0bagfH5bU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "sgdo_EYwB2EA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce77a99-57ea-4676-82e0-db2450915070"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677\n",
        "# Hyper-parameters\n",
        "embed_size = 128\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "num_epochs = 2\n",
        "num_samples = 1000     # number of words to be sampled\n",
        "batch_size = 30\n",
        "seq_length = 15 #number of elements in each batch from the dimension\n",
        "learning_rate = 0.002\n",
        "\n",
        "#load the corpus (dictionary it is)\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data(news_train_clean, batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "output_size = vocab_size\n",
        "num_batches = ids.size(1) // seq_length\n",
        "print(vocab_size)\n",
        "val_ids = corpus.get_data(news_validation_clean,batch_size)\n",
        "val_ids\n",
        "#print word2idx\n",
        "#print(corpus.dictionary.word2idx)"
      ],
      "metadata": {
        "id": "7migN3boCagt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d014bff0-7442-4773-93b6-4689461c2cc7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18226\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[    0,    53,    71,  ...,    17,  4476,     5],\n",
              "        [ 4056, 18246,     5,  ..., 18283,    14,     0],\n",
              "        [  518,     7,    22,  ...,   125,   749,  3740],\n",
              "        ...,\n",
              "        [  130,  5385,  3275,  ...,  7020,     7,   226],\n",
              "        [  142,    35,  4341,  ...,    86,   164,    28],\n",
              "        [ 3567,     3, 18994,  ...,     1,  2695,  1681]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, x, hidden):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, hidden)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        # out = self.softmax(out)\n",
        "        return out, (h, c)\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_size,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        return (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "                torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "model = LSTMModel(vocab_size, output_size, embed_size, hidden_size, num_layers)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "def detach (states):\n",
        "    return [state.detach() for state in states]"
      ],
      "metadata": {
        "id": "7Yw16Bo-hOfd"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states \n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    states = hidden\n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        # print(targets.shape)\n",
        "        # print(outputs.shape)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V2JzHB8m98m",
        "outputId": "b527eb79-f0e2-48cf-c638-bee356f3e826"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Step[0/1028], Loss: 9.8033, Perplexity: 18093.75\n",
            "Epoch [1/3], Step[100/1028], Loss: 6.0498, Perplexity: 424.03\n",
            "Epoch [1/3], Step[200/1028], Loss: 6.0108, Perplexity: 407.80\n",
            "Epoch [1/3], Step[300/1028], Loss: 5.6662, Perplexity: 288.95\n",
            "Epoch [1/3], Step[400/1028], Loss: 5.6959, Perplexity: 297.65\n",
            "Epoch [1/3], Step[500/1028], Loss: 5.3011, Perplexity: 200.56\n",
            "Epoch [1/3], Step[600/1028], Loss: 5.5745, Perplexity: 263.61\n",
            "Epoch [1/3], Step[700/1028], Loss: 5.9736, Perplexity: 392.92\n",
            "Epoch [1/3], Step[800/1028], Loss: 5.4877, Perplexity: 241.71\n",
            "Epoch [1/3], Step[900/1028], Loss: 5.4001, Perplexity: 221.44\n",
            "Epoch [1/3], Step[1000/1028], Loss: 5.4159, Perplexity: 224.95\n",
            "Epoch [2/3], Step[0/1028], Loss: 7.2309, Perplexity: 1381.46\n",
            "Epoch [2/3], Step[100/1028], Loss: 5.1877, Perplexity: 179.06\n",
            "Epoch [2/3], Step[200/1028], Loss: 5.1752, Perplexity: 176.83\n",
            "Epoch [2/3], Step[300/1028], Loss: 5.0319, Perplexity: 153.23\n",
            "Epoch [2/3], Step[400/1028], Loss: 5.1323, Perplexity: 169.41\n",
            "Epoch [2/3], Step[500/1028], Loss: 4.8151, Perplexity: 123.35\n",
            "Epoch [2/3], Step[600/1028], Loss: 5.2290, Perplexity: 186.60\n",
            "Epoch [2/3], Step[700/1028], Loss: 5.5632, Perplexity: 260.65\n",
            "Epoch [2/3], Step[800/1028], Loss: 4.9922, Perplexity: 147.26\n",
            "Epoch [2/3], Step[900/1028], Loss: 5.1483, Perplexity: 172.13\n",
            "Epoch [2/3], Step[1000/1028], Loss: 5.1473, Perplexity: 171.97\n",
            "Epoch [3/3], Step[0/1028], Loss: 5.5442, Perplexity: 255.75\n",
            "Epoch [3/3], Step[100/1028], Loss: 4.6537, Perplexity: 104.97\n",
            "Epoch [3/3], Step[200/1028], Loss: 4.7658, Perplexity: 117.42\n",
            "Epoch [3/3], Step[300/1028], Loss: 4.5090, Perplexity: 90.83\n",
            "Epoch [3/3], Step[400/1028], Loss: 4.7747, Perplexity: 118.47\n",
            "Epoch [3/3], Step[500/1028], Loss: 4.3992, Perplexity: 81.39\n",
            "Epoch [3/3], Step[600/1028], Loss: 4.7854, Perplexity: 119.75\n",
            "Epoch [3/3], Step[700/1028], Loss: 5.0864, Perplexity: 161.81\n",
            "Epoch [3/3], Step[800/1028], Loss: 4.5952, Perplexity: 99.01\n",
            "Epoch [3/3], Step[900/1028], Loss: 4.7339, Perplexity: 113.74\n",
            "Epoch [3/3], Step[1000/1028], Loss: 4.6917, Perplexity: 109.04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving best model checkpoint\n",
        "torch.save(model.state_dict(), 'fr_model.ckpt')"
      ],
      "metadata": {
        "id": "Ji9lec2XOSKZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculate perplexity of the input sentence using a model\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, sentence, vocab_size, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sentence = preprocess(sentence)\n",
        "        sentence = sentence.split()\n",
        "\n",
        "        #replace words in sentence with UNK\n",
        "        sentence = [word if word in corpus.dictionary.word2idx else '<UNK>' for word in sentence]\n",
        "        #convert the sentence to ids\n",
        "        ids = torch.tensor([[corpus.dictionary.word2idx[word] for word in sentence]])\n",
        "        ids = ids.to(device)\n",
        "        # Set initial hidden and cell states\n",
        "        states = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                    torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "        # Forward pass\n",
        "        for i in range(len(sentence)):\n",
        "            states = detach(states)\n",
        "            _, states = model(ids[:, i:i+1], states)\n",
        "        # Decode hidden states of all time steps\n",
        "        out = states[0][0].squeeze(0)\n",
        "        # for i in range(len(sentence)):\n",
        "        #     print('{}: {}'.format(sentence[i], F.softmax(out[i], dim=0)))\n",
        "    \n",
        "        #print probability of each word in the sentence\n",
        "        # for i in range(len(sentence)):\n",
        "        #     print('{}: {}'.format(sentence[i], out[i].item()))\n",
        "        #predict\n",
        "        #print perplexity score of the sentence\n",
        "        score = torch.exp(torch.log(torch.sum(torch.exp(out)))/len(sentence))\n",
        "        return score.item()\n",
        "  \n",
        "evaluate(model, 'Les journalistes, personnalités à forte capacité médiatique et politique, rapportent le plus.', vocab_size, device)\n"
      ],
      "metadata": {
        "id": "31DVkKyKLR_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d608e04-23f4-434e-eda6-5af212d8010d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3778657913208008"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pS-wmqgFnb53"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}