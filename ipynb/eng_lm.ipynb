{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eng_lm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7z4mGibSg2bM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48d3272c-c517-4ac4-cf11-8f3f14206184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys"
      ],
      "metadata": {
        "id": "2ZWelm6bjuKF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/europarl-corpus/train.europarl', 'r') as f:\n",
        "    europarl_train = f.readlines()\n",
        "with open('/content/drive/MyDrive/europarl-corpus/dev.europarl', 'r') as f:\n",
        "    europarl_validation = f.readlines()\n",
        "with open('/content/drive/MyDrive/europarl-corpus/test.europarl', 'r') as f:\n",
        "    europarl_test = f.readlines()\n"
      ],
      "metadata": {
        "id": "p2HktpoilUAF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print length of corpus\n",
        "print('train:', len(europarl_train))  \n",
        "print('valid:', len(europarl_validation))"
      ],
      "metadata": {
        "id": "L7L1raBKSMlG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf3feade-894a-471e-c940-8504b9c99ff1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: 20000\n",
            "valid: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def preprocess(text):\n",
        "    #the function prpocess a single line\n",
        "    #it takes the single line and returns a list of tokens for that particular line\n",
        "    #make text lower\n",
        "    cleaned_text = text.lower()\n",
        "    # cleaned_text = text\n",
        "    #remove non-ASCII characters\n",
        "    #cleaned_text = re.sub(r'[^\\x00-\\x7F]+',' ', cleaned_text)\n",
        "    # remove URLS\n",
        "    cleaned_text = re.sub(r\"http\\S+\", \"<URL>\", cleaned_text)\n",
        "    # remove HTs\n",
        "    cleaned_text = re.sub(r\"#[A-Za-z0-9_]+\", \"<HASHTAG>\", cleaned_text)\n",
        "    # remove Mentions\n",
        "    cleaned_text = re.sub(r\"@[A-Za-z0-9_]+\", \"<MENTION>\", cleaned_text)\n",
        "    #replace percentage quantities with tags\n",
        "    cleaned_text = re.sub(r'(\\d+(\\.\\d+)?%)',\"<PERCENT>\",cleaned_text)\n",
        "    #replace numbers with tags\n",
        "    cleaned_text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" <NUM> \", cleaned_text)\n",
        "    #hypenated words are accounted for by joining them/merging them together\n",
        "    cleaned_text = re.sub(r'\\w+(?:-\\w+)+', '', cleaned_text)\n",
        "    # Substitue for punctuations\n",
        "    cleaned_text = re.sub(r\"(\\'t)\",\" not\",cleaned_text)\n",
        "    cleaned_text = re.sub(r'(i\\'m)',\"i am\",cleaned_text)\n",
        "    cleaned_text = re.sub(r'(ain\\'t)',\"am not\",cleaned_text)\n",
        "    cleaned_text = re.sub(r'(\\'ll)',\" will\",cleaned_text)\n",
        "    cleaned_text = re.sub(r'(\\'ve)',\" have\",cleaned_text)\n",
        "    cleaned_text = re.sub(r'(\\'re)',\" are\",cleaned_text)\n",
        "    cleaned_text = re.sub(r'(\\'s)',\" is\",cleaned_text)\n",
        "    cleaned_text = re.sub(r'(\\'re)',\" are\",cleaned_text)\n",
        "    #removing repetetive spam\n",
        "    cleaned_text = re.sub('\\!\\!+', '!', cleaned_text)\n",
        "    cleaned_text = re.sub('\\*\\*+', '*', cleaned_text)\n",
        "    cleaned_text = re.sub('\\>\\>+', '>', cleaned_text)\n",
        "    cleaned_text = re.sub('\\<\\<+', '<', cleaned_text)\n",
        "    cleaned_text = re.sub('\\?\\?+', '?', cleaned_text)\n",
        "    cleaned_text = re.sub('\\!\\!+', '!', cleaned_text)\n",
        "    cleaned_text = re.sub('\\.\\.+', '.', cleaned_text)\n",
        "    cleaned_text = re.sub('\\,\\,+', ',', cleaned_text)\n",
        "    #matching punctuation characters at end of sentences and padding them\n",
        "    cleaned_text = re.sub('([;:.,!?()])', r' \\1 ', cleaned_text)\n",
        "    #removing multiple spaces finally\n",
        "    cleaned_text = re.sub('\\s{2,}', ' ', cleaned_text)\n",
        "    #remove trailing white spaces\n",
        "    cleaned_text = re.sub(r'\\s+$', '', cleaned_text) #important to get rid of empty tokens at the end of list\n",
        "    #tokenization based on spaces for each line\n",
        "    # spaces = r\"\\s+\"\n",
        "    # tokenized_sent = re.split(spaces, cleaned_text)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "x15Gaay6dj9Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess the corpus\n",
        "europarl_train_clean = [preprocess(line) for line in europarl_train]\n",
        "europarl_validation_clean = [preprocess(line) for line in europarl_validation]\n",
        "europarl_validation_clean[:10]"
      ],
      "metadata": {
        "id": "j-pLI43ldlpL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15618180-b5df-4869-965e-697dd7d76c68"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it is this tendency which presents a problem right now , much more than the scale of the epidemic in quantitative terms .',\n",
              " 'the truth is , at the present time , there is quite simply no adequate explanation for it , hence the debate which has been initiated on the possible existence of alternative disease transmission routes of which we are , as yet , unaware .',\n",
              " 'medical and scientific uncertainty still prevails .',\n",
              " 'in these circumstances , the precautionary principle must be adopted to the full .',\n",
              " 'all possible resources must be used in order to assess the bse situation in the various countries , particularly including the development and systematic use of fast screening tests .',\n",
              " 'the development of bse in france raises issues that are not purely medical in the narrowest sense .',\n",
              " 'this is nothing new .',\n",
              " 'from the very beginning , it has not been possible to give any explanation for the emergence of this disease and its transmission to human beings , and its spread internationally , without taking the key factors into account : contemporary trends in the , the subordination of producers and consumers to the dominating rationale of capitalistic profit .',\n",
              " 'still today , the way in which the is organised , and the fact that it is infiltrated by international networks , make the fight to eradicate bse and tses even more difficult .',\n",
              " 'no public health policy should ignore these factors .']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "europarl_test_clean = [preprocess(line) for line in europarl_test]"
      ],
      "metadata": {
        "id": "v7IvYK9TQMv-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_UNK_corpus(data):\n",
        "    freq_dict = {}\n",
        "    for line in data:\n",
        "        for word in line.split():\n",
        "            if word in freq_dict:\n",
        "                freq_dict[word] += 1\n",
        "            else:\n",
        "                freq_dict[word] = 1\n",
        "    freq_dict['<UNK>'] = 0\n",
        "    #replace all words with frequency less than 3 with <UNK> in freq_dict\n",
        "    word_list = []\n",
        "    for word in freq_dict:\n",
        "        if freq_dict[word] == 1:\n",
        "            freq_dict[\"<UNK>\"] += freq_dict[word]\n",
        "            #remove the word from the freq_dict\n",
        "            word_list.append(word)\n",
        "    for word in word_list:\n",
        "        del freq_dict[word]\n",
        "    #for each sentence in corpus if word not in freq_dict replace with <UNK>\n",
        "    temp = []\n",
        "    for line in data:\n",
        "        for word in line.split():\n",
        "            if word not in freq_dict:\n",
        "                line = line.replace(word, \"<UNK>\")\n",
        "        temp.append(line)\n",
        "    return temp\n",
        "europarl_train_clean = create_UNK_corpus(europarl_train_clean)\n",
        "europarl_train_clean[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5xQsgoy28nT",
        "outputId": "2d6b5c6f-eccf-448c-9042-4a754bfa3f13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['resumption of the session',\n",
              " 'i declare resumed the session of the european parliament adjourned on friday <NUM> december 1999 , and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period .',\n",
              " 'although , as you will have seen , the <UNK> <UNK> <UNK> failed to materialise , still the people in a number of countries suffered a series of natural disasters that truly were dreadful .',\n",
              " 'you have requested a debate on this subject in the course of the next few days , during this .',\n",
              " \"in the meantime , i should like to observe a minute' s silence , as a number of members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , in the various countries of the european union .\",\n",
              " \"please rise , then , for this minute' s silence .\",\n",
              " \" ( the house rose and observed a minute' s silence )\",\n",
              " 'madam president , on a point of order .',\n",
              " 'you will be aware from the press and television that there have been a number of bomb <UNK> and killings in sri lanka .',\n",
              " 'one of the people assassinated very recently in sri lanka was mr <UNK> <UNK> , who had visited the european parliament just a few months ago .']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "class Dictionary(object):\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = []\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.idx2word.append(word)\n",
        "            self.word2idx[word] = len(self.idx2word) - 1\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.idx2word)\n",
        "\n",
        "class Corpus(object):\n",
        "    def __init__(self):\n",
        "        self.dictionary = Dictionary()\n",
        "    def get_data(self, train_data, batch_size):\n",
        "        tokens = 0\n",
        "        for line in train_data:\n",
        "            #tokenize the line\n",
        "            words = ['<START>'] + line.split() + ['<END>']\n",
        "            tokens += len(words)\n",
        "            for word in words:\n",
        "                self.dictionary.add_word(word)\n",
        "\n",
        "        #tokenizing the content\n",
        "        ids = torch.LongTensor(tokens) #contains the ids of the words\n",
        "        token = 0\n",
        "        for line in train_data:\n",
        "            words = ['<START>'] + line.split() + ['<END>']\n",
        "            for word in words:\n",
        "                ids[token] = self.dictionary.word2idx[word]\n",
        "                token += 1\n",
        "        num_batches = ids.size(0) // batch_size\n",
        "        ids = ids[:num_batches*batch_size]\n",
        "        ##return reshaped tensor using .view()\n",
        "        ## the -1-> situation that you don't know how many rows you want but are sure of the number of columns\n",
        "        return ids.view(batch_size, -1) "
      ],
      "metadata": {
        "id": "0fh0bagfH5bU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "id": "sgdo_EYwB2EA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490d0923-45d1-45e5-f997-e3f301a2ec1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677\n",
        "# Hyper-parameters\n",
        "embed_size = 128\n",
        "hidden_size = 1024\n",
        "num_layers = 2\n",
        "num_epochs = 2\n",
        "num_samples = 1000     # number of words to be sampled\n",
        "batch_size = 30\n",
        "seq_length = 15 #number of elements in each batch from the dimension\n",
        "learning_rate = 0.002\n",
        "\n",
        "#load the corpus (dictionary it is)\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data(europarl_train_clean, batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "output_size = vocab_size\n",
        "num_batches = ids.size(1) // seq_length\n",
        "print(vocab_size)\n",
        "val_ids = corpus.get_data(europarl_validation_clean,batch_size)\n",
        "val_ids\n",
        "#print word2idx\n",
        "#print(corpus.dictionary.word2idx)"
      ],
      "metadata": {
        "id": "7migN3boCagt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d39a84-587c-4365-bd96-d88543700dc8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9408\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,  119,  124,  ...,  348,   17,  119],\n",
              "        [  19,   96, 8268,  ..., 1062,   37,    5],\n",
              "        [   0,    6,   67,  ...,   19,   17,    2],\n",
              "        ...,\n",
              "        [ 330,  326, 1294,  ...,   30,  245, 2840],\n",
              "        [  18,   26,  732,  ...,    2,    3,    9],\n",
              "        [  10,   17,  313,  ...,  126,  954,  201]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embed_size, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "    def forward(self, x, hidden):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, hidden)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        # out = self.softmax(out)\n",
        "        return out, (h, c)\n",
        "    def init_hidden(self, batch_size):\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_size,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        return (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "                torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "model = LSTMModel(vocab_size, output_size, embed_size, hidden_size, num_layers)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "def detach (states):\n",
        "    return [state.detach() for state in states]"
      ],
      "metadata": {
        "id": "7Yw16Bo-hOfd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states \n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    states = hidden\n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        # print(targets.shape)\n",
        "        # print(outputs.shape)\n",
        "        # print(outputs)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V2JzHB8m98m",
        "outputId": "a16ad2d5-d229-4167-c8ed-356e97e04ce3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/3], Step[0/1331], Loss: 9.1515, Perplexity: 9428.30\n",
            "Epoch [1/3], Step[100/1331], Loss: 5.5918, Perplexity: 268.23\n",
            "Epoch [1/3], Step[200/1331], Loss: 5.8972, Perplexity: 364.03\n",
            "Epoch [1/3], Step[300/1331], Loss: 5.1784, Perplexity: 177.39\n",
            "Epoch [1/3], Step[400/1331], Loss: 5.1839, Perplexity: 178.37\n",
            "Epoch [1/3], Step[500/1331], Loss: 5.2222, Perplexity: 185.34\n",
            "Epoch [1/3], Step[600/1331], Loss: 5.0215, Perplexity: 151.63\n",
            "Epoch [1/3], Step[700/1331], Loss: 5.1533, Perplexity: 172.99\n",
            "Epoch [1/3], Step[800/1331], Loss: 4.9195, Perplexity: 136.93\n",
            "Epoch [1/3], Step[900/1331], Loss: 5.1278, Perplexity: 168.65\n",
            "Epoch [1/3], Step[1000/1331], Loss: 5.1840, Perplexity: 178.40\n",
            "Epoch [1/3], Step[1100/1331], Loss: 4.8878, Perplexity: 132.66\n",
            "Epoch [1/3], Step[1200/1331], Loss: 5.0884, Perplexity: 162.13\n",
            "Epoch [1/3], Step[1300/1331], Loss: 5.0556, Perplexity: 156.90\n",
            "Epoch [2/3], Step[0/1331], Loss: 5.9643, Perplexity: 389.26\n",
            "Epoch [2/3], Step[100/1331], Loss: 4.6807, Perplexity: 107.85\n",
            "Epoch [2/3], Step[200/1331], Loss: 5.4549, Perplexity: 233.91\n",
            "Epoch [2/3], Step[300/1331], Loss: 4.5988, Perplexity: 99.36\n",
            "Epoch [2/3], Step[400/1331], Loss: 4.7848, Perplexity: 119.68\n",
            "Epoch [2/3], Step[500/1331], Loss: 4.8974, Perplexity: 133.95\n",
            "Epoch [2/3], Step[600/1331], Loss: 4.5892, Perplexity: 98.41\n",
            "Epoch [2/3], Step[700/1331], Loss: 4.6975, Perplexity: 109.68\n",
            "Epoch [2/3], Step[800/1331], Loss: 4.4539, Perplexity: 85.96\n",
            "Epoch [2/3], Step[900/1331], Loss: 4.6784, Perplexity: 107.60\n",
            "Epoch [2/3], Step[1000/1331], Loss: 4.7730, Perplexity: 118.27\n",
            "Epoch [2/3], Step[1100/1331], Loss: 4.4517, Perplexity: 85.78\n",
            "Epoch [2/3], Step[1200/1331], Loss: 4.7813, Perplexity: 119.26\n",
            "Epoch [2/3], Step[1300/1331], Loss: 4.7633, Perplexity: 117.13\n",
            "Epoch [3/3], Step[0/1331], Loss: 5.0226, Perplexity: 151.81\n",
            "Epoch [3/3], Step[100/1331], Loss: 4.5134, Perplexity: 91.23\n",
            "Epoch [3/3], Step[200/1331], Loss: 5.3653, Perplexity: 213.85\n",
            "Epoch [3/3], Step[300/1331], Loss: 4.4914, Perplexity: 89.25\n",
            "Epoch [3/3], Step[400/1331], Loss: 4.7612, Perplexity: 116.88\n",
            "Epoch [3/3], Step[500/1331], Loss: 4.8097, Perplexity: 122.70\n",
            "Epoch [3/3], Step[600/1331], Loss: 4.6043, Perplexity: 99.91\n",
            "Epoch [3/3], Step[700/1331], Loss: 4.6160, Perplexity: 101.09\n",
            "Epoch [3/3], Step[800/1331], Loss: 4.3997, Perplexity: 81.42\n",
            "Epoch [3/3], Step[900/1331], Loss: 4.6922, Perplexity: 109.09\n",
            "Epoch [3/3], Step[1000/1331], Loss: 4.7923, Perplexity: 120.58\n",
            "Epoch [3/3], Step[1100/1331], Loss: 4.4030, Perplexity: 81.70\n",
            "Epoch [3/3], Step[1200/1331], Loss: 4.7135, Perplexity: 111.44\n",
            "Epoch [3/3], Step[1300/1331], Loss: 4.6268, Perplexity: 102.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#saving best model checkpoint\n",
        "torch.save(model.state_dict(), 'eng_model.ckpt')"
      ],
      "metadata": {
        "id": "Ji9lec2XOSKZ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#calculate perplexity of the input sentence using a model\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def evaluate(model, sentence, vocab_size, device):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sentence = preprocess(sentence)\n",
        "        sentence = sentence.split()\n",
        "\n",
        "        #replace words in sentence with UNK\n",
        "        sentence = [word if word in corpus.dictionary.word2idx else '<UNK>' for word in sentence]\n",
        "        #convert the sentence to ids\n",
        "        ids = torch.tensor([[corpus.dictionary.word2idx[word] for word in sentence]])\n",
        "        ids = ids.to(device)\n",
        "        # Set initial hidden and cell states\n",
        "        states = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                    torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "        # Forward pass\n",
        "        for i in range(len(sentence)):\n",
        "            states = detach(states)\n",
        "            _, states = model(ids[:, i:i+1], states)\n",
        "        # Decode hidden states of all time steps\n",
        "        out = states[0][0].squeeze(0)\n",
        "        # for i in range(len(sentence)):\n",
        "        #     print('{}: {}'.format(sentence[i], F.softmax(out[i], dim=0)))\n",
        "    \n",
        "        #print probability of each word in the sentence\n",
        "        # for i in range(len(sentence)):\n",
        "        #     print('{}: {}'.format(sentence[i], out[i].item()))\n",
        "        #predict\n",
        "        #print perplexity score of the sentence\n",
        "        score = torch.exp(torch.log(torch.sum(torch.exp(out)))/len(sentence))\n",
        "        return score.item()\n",
        "  \n",
        "evaluate(model, 'What is more, the movement of fish is like CO2 or like capital, it is not European, but world-wide.', vocab_size, device)\n"
      ],
      "metadata": {
        "id": "31DVkKyKLR_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d608e04-23f4-434e-eda6-5af212d8010d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3778657913208008"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pS-wmqgFnb53"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}